{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%reload_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from ibm_watson_machine_learning import APIClient\n","import pandas as pd"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Create a client instance"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["'1.0.308'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["url = \"https://us-south.ml.cloud.ibm.com\"\n","apikey = \"X\"\n","\n","\n","wml_credentials = {\n","      \"apikey\": apikey,\n","      \"url\": url\n","}\n","\n","client = APIClient(wml_credentials)\n","client.version"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Prepare your model archive"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["rmdir: failed to remove 'model': No such file or directory\n","rm: cannot remove 'model.py': No such file or directory\n","rm: cannot remove 'model': No such file or directory\n"]}],"source":["try:\n","    !rmdir model\n","except:\n","    True\n","     \n","try:\n","    !rm model.py\n","except:\n","    True\n","try:    \n","    !rm -r model  \n","except:\n","    True\n","try:    \n","    !rm model.tar.gz\n","except:\n","    True    \n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["     \n","!ls"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["%mkdir model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing model/main.py\n"]}],"source":["%%writefile model/main.py\n","\n","from docplex.util.environment import get_environment\n","import pandas\n","from six import iteritems\n","from collections.abc import Mapping\n","from os.path import join, dirname, basename, splitext, exists\n","import glob\n","\n","class _InputDict(dict):\n","    def __init__(self, directory, names):\n","        dict.__init__(self)\n","        self._directory = directory\n","        for k in names:\n","            dict.__setitem__(self, k, None)\n","        file='model_schema.json'\n","        if self._directory is not None:\n","            file  = \"{0}/\".format(self._directory) + file\n","        self.dtype_schemas = self.get_dtype_schemas( file)\n","    def __getitem__(self, key):\n","        if isinstance(key, str):\n","            item = dict.__getitem__(self, key)\n","            if item is None:\n","                file = \"{0}.csv\".format(key)\n","                if file in self.dtype_schemas:\n","                    return self.read_df( key, dtype=self.dtype_schemas[file])\n","                else:\n","                    return self.read_df( key)\n","            else:\n","                return item\n","        else:\n","            raise Exception(\"Accessing input dict via non string index\")\n","    def read_df(self, key, **kwargs):\n","        env = get_environment()\n","        file = \"{0}.csv\".format(key)\n","        if self._directory is not None:\n","            file  = \"{0}/\".format(self._directory) + file\n","        with env.get_input_stream(file) as ist:\n","            params = {'encoding': 'utf8'}\n","            if kwargs:\n","                params.update(kwargs)\n","            df = pandas.read_csv( ist, **params)\n","            dict.__setitem__(self, key, df)\n","        return df\n","    def get_dtype_schemas(self, path):\n","        dtype_schemas = {}\n","        if exists(path):\n","            input_schemas=json.load(open(path))\n","            if 'input' in input_schemas:\n","                for input_schema in input_schemas['input']:\n","                    dtype_schema = {}\n","                    if 'fields' in input_schema:\n","                        for input_schema_field in input_schema['fields']:\n","                            if input_schema_field['type']=='string':\n","                                dtype_schema[input_schema_field['name']]='str'\n","                        if len(dtype_schema) > 0:\n","                            dtype_schemas[input_schema['id']]=dtype_schema\n","        print(dtype_schemas)\n","        return dtype_schemas\n","\n","class _LazyDict(Mapping):\n","    def __init__(self, *args, **kw):\n","        self._raw_dict = _InputDict(*args, **kw)\n","\n","    def __getitem__(self, key):\n","        return self._raw_dict.__getitem__(key)\n","\n","    def __iter__(self):\n","        return iter(self._raw_dict)\n","\n","    def __len__(self):\n","        return len(self._raw_dict)\n","\n","    def read_df(self, key, **kwargs):\n","        return self._raw_dict.read_df(key, **kwargs)\n","\n","def get_all_inputs(directory=None):\n","    '''Utility method to read a list of files and return a tuple with all\n","    read data frames.\n","    Returns:\n","        a map { datasetname: data frame }\n","    '''\n","\n","    all_csv = \"*.csv\"\n","    g = join(directory, all_csv) if directory else all_csv\n","\n","    names = [splitext(basename(f))[0] for f in glob.glob(g)]\n","    result = _LazyDict(directory, names)\n","    return result\n","\n","def write_all_outputs(outputs):\n","    '''Write all dataframes in ``outputs`` as .csv.\n","\n","    Args:\n","        outputs: The map of outputs 'outputname' -> 'output df'\n","    '''\n","    for (name, df) in iteritems(outputs):\n","        csv_file = '%s.csv' % name\n","        print(csv_file)\n","        with get_environment().get_output_stream(csv_file) as fp:\n","            if sys.version_info[0] < 3:\n","                fp.write(df.to_csv(index=False, encoding='utf8'))\n","            else:\n","                fp.write(df.to_csv(index=False).encode(encoding='utf8'))\n","    if len(outputs) == 0:\n","        print(\"Warning: no outputs written\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to model/main.py\n"]}],"source":["%%writefile -a model/main.py\n","\n","import pandas as pd\n","from docplex.mp.model import Model\n","\n","def binary_var_series(df, mdl,**kargs):\n","    return pd.Series(mdl.binary_var_list(df.index, **kargs), index = df.index)\n","\n","\n","\n","class CplexSum():\n","    \"\"\"Function class that adds a series of dvars into a cplex sum expression.\n","    To be used as a custom aggregation in a groupby.\n","    Usage:\n","        df2 = df1.groupby(['a']).agg({'xDVar':CplexSum(engine.mdl)}).rename(columns={'xDVar':'expr'})\n","\n","    Sums the dvars in the 'xDVar' column into an expression\n","    \"\"\"\n","    def __init__(self, mdl):\n","        self.mdl = mdl\n","    def __call__(self, dvar_series):\n","        return self.mdl.sum(dvar_series)\n","\n","\n","def extract_solution(df, extract_dvar_names=None, drop_column_names=None, drop:bool=True):\n","    df = df.copy()\n","    \"\"\"Generalized routine to extract a solution value. \n","    Can remove the dvar column from the df to be able to have a clean df for export into scenario.\"\"\"\n","    if extract_dvar_names is not None:\n","        for xDVarName in extract_dvar_names:\n","            if xDVarName in df.columns:\n","                df[f'{xDVarName}_Solution'] = [dvar.solution_value for dvar in df[xDVarName]]\n","                if drop:\n","                    df = df.drop([xDVarName], axis = 1)\n","    if drop and drop_column_names is not None:\n","        for column in drop_column_names:\n","            if column in df.columns:\n","                df = df.drop([column], axis = 1)\n","    return df   \n","\n","inputs = get_all_inputs()\n","\n","Nurse = inputs['Nurse']\n","Days = inputs['Days']\n","Shifts = inputs['Shifts']\n","\n","\n","MODEL_NAME = 'NurseScheduling'\n","SCENARIO_NAME = 'Base_Scenario' \n","\n","mdl = Model(MODEL_NAME)\n","\n","\n","Nurse_Shift = pd.merge(Days,Shifts, how = \"cross\" ).merge(Nurse, how = \"cross\").set_index([\"DayID\",\"ShiftsID\",\"NurseID\"], verify_integrity = True)\n","Nurse_Shift[\"X_Assigned\"] = binary_var_series(Nurse_Shift, mdl, name = \"X_Assigned\" )\n","Nurse_Shift = Nurse_Shift.reset_index(drop = False)\n","\n","Nurse_each_shift = Nurse_Shift[[\"DayID\",\"ShiftsID\",\"X_Assigned\"]].groupby([\"DayID\",\"ShiftsID\"]).agg(CplexSum(mdl)).reset_index(drop= False)\n","\n","\n","for row in Nurse_each_shift.itertuples():\n","    mdl.add_constraint(row.X_Assigned  == 1)\n","\n","Nurse_each_day = Nurse_Shift[[\"NurseID\",\"DayID\",\"X_Assigned\"]].groupby([\"NurseID\",\"DayID\"]).agg(CplexSum(mdl)).reset_index(drop= False)\n","\n","for row in Nurse_each_day.itertuples():\n","    mdl.add_constraint(row.X_Assigned  <= 1)\n","\n","Nurse_day = Nurse_Shift[[\"NurseID\",\"X_Assigned\"]].groupby([\"NurseID\"]).agg(CplexSum(mdl)).reset_index(drop= False)\n","\n","for row in Nurse_day.itertuples():\n","    mdl.add_constraint(row.X_Assigned  >= 2)\n","\n","ok = mdl.solve(log_output=True)\n","\n","if ok:\n","    mdl.print_solution()\n","\n","\n","    solution = extract_solution(Nurse_Shift, extract_dvar_names= ['X_Assigned'] ,drop=False)\n","    solution = solution[solution[\"X_Assigned_Solution\"]>0.1]\n","    Schedule = solution.pivot(index = \"DayID\", columns = \"ShiftsID\", values = \"NurseID\")\n","    print(Schedule)\n","    outputs={}\n","    outputs['Schedule']=Schedule\n","    \n","# Generate output files\n","write_all_outputs(outputs)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model\r\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import tarfile\n","def reset(tarinfo):\n","    tarinfo.uid = tarinfo.gid = 0\n","    tarinfo.uname = tarinfo.gname = \"root\"\n","    return tarinfo\n","tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n","#tar.add(\"model.py\", arcname=\"model.py\", filter=reset)\n","tar.add(\"model/main.py\", arcname=\"main.py\", filter=reset)\n","tar.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Upload your model on Watson Machine Learning"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["'SUCCESS'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Find the space ID\n","\n","space_name = 'Deployment_space_DO'\n","\n","space_id = [x['metadata']['id'] for x in client.spaces.get_details()['resources'] if x['entity']['name'] == space_name][0]\n","\n","client.set.default_space(space_id)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["mnist_metadata = {\n","    client.repository.ModelMetaNames.NAME: \"MyModel\",\n","    client.repository.ModelMetaNames.DESCRIPTION: \"Model for Nurse Scheduling\",\n","    client.repository.ModelMetaNames.TYPE: \"do-docplex_22.1\",\n","    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_uid_by_name(\"do_22.1\"),\n","}\n","\n","model_details = client.repository.store_model(model='/home/wsuser/work/model.tar.gz', meta_props=mnist_metadata)\n","#model='/home/wsuser/work/model.tar.gz', \n","model_uid = client.repository.get_model_id(model_details)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Create a deployment"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","#######################################################################################\n","\n","Synchronous deployment creation for uid: '3f8e6c22-5908-4d07-b4ae-ded639c5f516' started\n","\n","#######################################################################################\n","\n","\n","ready.\n","\n","\n","------------------------------------------------------------------------------------------------\n","Successfully finished deployment creation, deployment_uid='3e355bf1-2a5e-435d-816d-5a9e300228f8'\n","------------------------------------------------------------------------------------------------\n","\n","\n","------------------------------------  --------------------  -----  ------------------------  -------------  ----------  ----------------\n","GUID                                  NAME                  STATE  CREATED                   ARTIFACT_TYPE  SPEC_STATE  SPEC_REPLACEMENT\n","3e355bf1-2a5e-435d-816d-5a9e300228f8  Nurse Scheduling      ready  2023-06-30T02:21:59.012Z  model          supported\n","3d6d1f12-4ef6-4dea-889a-c1a9b619f46f  NurseModelDeployment  ready  2023-06-30T01:45:50.669Z  model          supported\n","------------------------------------  --------------------  -----  ------------------------  -------------  ----------  ----------------\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>GUID</th>\n","      <th>NAME</th>\n","      <th>STATE</th>\n","      <th>CREATED</th>\n","      <th>ARTIFACT_TYPE</th>\n","      <th>SPEC_STATE</th>\n","      <th>SPEC_REPLACEMENT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3e355bf1-2a5e-435d-816d-5a9e300228f8</td>\n","      <td>Nurse Scheduling</td>\n","      <td>ready</td>\n","      <td>2023-06-30T02:21:59.012Z</td>\n","      <td>model</td>\n","      <td>supported</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3d6d1f12-4ef6-4dea-889a-c1a9b619f46f</td>\n","      <td>NurseModelDeployment</td>\n","      <td>ready</td>\n","      <td>2023-06-30T01:45:50.669Z</td>\n","      <td>model</td>\n","      <td>supported</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   GUID                  NAME  STATE  \\\n","0  3e355bf1-2a5e-435d-816d-5a9e300228f8      Nurse Scheduling  ready   \n","1  3d6d1f12-4ef6-4dea-889a-c1a9b619f46f  NurseModelDeployment  ready   \n","\n","                    CREATED ARTIFACT_TYPE SPEC_STATE SPEC_REPLACEMENT  \n","0  2023-06-30T02:21:59.012Z         model  supported                   \n","1  2023-06-30T01:45:50.669Z         model  supported                   "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["meta_props = {\n","    client.deployments.ConfigurationMetaNames.NAME: \"Nurse Scheduling\",\n","    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"Nurse Scheduling\",\n","    client.deployments.ConfigurationMetaNames.BATCH: {},\n","    client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: {'name': 'S', 'num_nodes': 1}\n","}\n","\n","deployment_details = client.deployments.create(model_uid, meta_props=meta_props)\n","\n","deployment_uid = client.deployments.get_uid(deployment_details)\n","\n","client.deployments.list()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["'3e355bf1-2a5e-435d-816d-5a9e300228f8'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["deployment_uid"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Create another data and test with the model using a job"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import pandas as pd \n","  \n","Number_Nurses = 4\n","Number_Days = 3\n","Number_Shifts = 3\n","\n","Nurse = pd.DataFrame([])\n","Nurse[\"NurseID\"] = [\"Nurse\" +str(i) for i in range(1,Number_Nurses+1)]\n","\n","Days = pd.DataFrame([])\n","Days[\"DayID\"] = [\"Day\" +str(i) for i in range(1,Number_Days+1)]\n","\n","Shifts = pd.DataFrame([])\n","Shifts[\"ShiftsID\"] = [\"Shifts\" +str(i) for i in range(1,Number_Shifts+1)]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["'46eb0ccb-0b0c-4e5c-a401-9da232732bd9'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["solve_payload = {\n","    client.deployments.DecisionOptimizationMetaNames.INPUT_DATA: [{\n","            \"id\":\"Nurse.csv\",\n","            \"values\" : Nurse\n","        },\n","        {\n","            \"id\":\"Days.csv\",\n","            \"values\" : Days\n","        },\n","        {\n","            \"id\":\"Shifts.csv\",\n","            \"values\" : Shifts\n","        }\n","    ],\n","    client.deployments.DecisionOptimizationMetaNames.OUTPUT_DATA: [\n","    {\n","        \"id\":\".*\\.csv\"\n","    }\n","    ]\n","}\n","job_details = client.deployments.create_job(deployment_uid, solve_payload)\n","job_uid = client.deployments.get_job_uid(job_details)\n","job_uid"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["queued...\n","queued...\n","queued...\n","running...\n","completed\n"]}],"source":["from time import sleep\n","\n","while job_details['entity']['decision_optimization']['status']['state'] not in ['completed', 'failed', 'canceled']:\n","    print(job_details['entity']['decision_optimization']['status']['state'] + '...')\n","    sleep(5)\n","    job_details=client.deployments.get_job_details(job_uid)\n","\n","print( job_details['entity']['decision_optimization']['status']['state'])"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["[{'fields': ['Shifts1', 'Shifts2', 'Shifts3'],\n","  'id': 'Schedule.csv',\n","  'values': [['Nurse3', 'Nurse1', 'Nurse4'],\n","   ['Nurse3', 'Nurse2', 'Nurse1'],\n","   ['Nurse2', 'Nurse3', 'Nurse4']]},\n"," {'fields': ['Name', 'Value'],\n","  'id': 'stats.csv',\n","  'values': [['cplex.modelType', 'MILP'],\n","   ['cplex.size.integerVariables', 0],\n","   ['cplex.size.continousVariables', 0],\n","   ['cplex.size.linearConstraints', 25],\n","   ['cplex.size.booleanVariables', 36],\n","   ['cplex.size.constraints', 25],\n","   ['cplex.size.quadraticConstraints', 0],\n","   ['cplex.size.variables', 36],\n","   ['job.coresCount', 1],\n","   ['job.inputsReadMs', 2161],\n","   ['job.memoryPeakKB', 100852],\n","   ['job.modelProcessingMs', 357]]}]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["job_details['entity']['decision_optimization']['output_data']"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'completed_at': '2023-06-30T02:22:28.662Z', 'running_at': '2023-06-30T02:22:25.925Z', 'state': 'completed'}\n"]}],"source":["print(client.deployments.get_job_details(job_uid)['entity']['decision_optimization']['status'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# delete the deployment"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["'SUCCESS'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["client.deployments.delete(deployment_uid)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_data': [{'id': 'Nurse.csv',\n","   'values': [['Nurse1'], ['Nurse2'], ['Nurse3'], ['Nurse4']],\n","   'fields': ['NurseID']},\n","  {'id': 'Days.csv',\n","   'values': [['Day1'], ['Day2'], ['Day3']],\n","   'fields': ['DayID']},\n","  {'id': 'Shifts.csv',\n","   'values': [['Shifts1'], ['Shifts2'], ['Shifts3']],\n","   'fields': ['ShiftsID']}],\n"," 'output_data': [{'id': '.*\\\\.csv'}]}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["solve_payload"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":1}
